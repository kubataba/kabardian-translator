#!/usr/bin/env python3
"""
–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–µ—Ä–µ–≤–æ–¥–∞ RU‚ÜîKBD
–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –≤–∞—à–∏ Opus-MT –º–æ–¥–µ–ª–∏ —Å M2M100 –º–æ–¥–µ–ª—è–º–∏ –Ω–∞ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö

–ú–µ—Ç—Ä–∏–∫–∏:
- BLEU (SacreBLEU)
- chrF/chrF++
- TER (Translation Error Rate)
- –°–∫–æ—Ä–æ—Å—Ç—å –ø–µ—Ä–µ–≤–æ–¥–∞
- –ü—Ä–∏–º–µ—Ä—ã –ø–µ—Ä–µ–≤–æ–¥–æ–≤

License: CC BY-NC 4.0
Author: [–í–∞—à–µ –∏–º—è]
"""

import torch
from transformers import (
    AutoTokenizer, AutoModelForSeq2SeqLM,
    MarianTokenizer, MarianMTModel
)
from datasets import load_from_disk
import evaluate
from pathlib import Path
import time
import json
import pandas as pd
from datetime import datetime
import random
import numpy as np

print("=" * 80)
print("üèÜ –°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ï–ô –ü–ï–†–ï–í–û–î–ê RU‚ÜîKBD")
print("=" * 80)

# ============================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# ============================================================================

# –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
device = "mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu"
print(f"üíª –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç–µ—Å—Ç–∞
TEST_SIZE = 1000  # 1000 –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
SEED = 42
SAMPLE_INTERVAL = 50  # –ë–µ—Ä–µ–º –∫–∞–∂–¥–æ–µ 50-–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

# ============================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ú–û–î–ï–õ–ï–ô (–ì–†–£–ü–ü–ò–†–û–í–ö–ê –ü–û –ù–ê–ü–†–ê–í–õ–ï–ù–ò–Ø–ú)
# ============================================================================

MODELS = {
    "ru_kbd": {
        "opus_ru_kbd": {
            "name": "Opus-MT RU‚ÜíKBD (kubataba)",
            "type": "opus",
            "path": "models/opus-mt-ru-kbd",
            "base_model": "Helsinki-NLP/opus-mt-ru-uk",
            "author": "kubataba",
            "direction": "ru_kbd"
        }   
    },
    "kbd_ru": {
        "opus_kbd_ru": {
            "name": "Opus-MT KBD‚ÜíRU (kubataba)",
            "type": "opus",
            "path": "models/opus-mt-kbd-ru",
            "base_model": "Helsinki-NLP/opus-mt-en-ru",
            "author": "kubataba",
            "direction": "kbd_ru"
        }
    }
}

# ============================================================================
# –§–£–ù–ö–¶–ò–ò –ü–†–ï–ü–†–û–¶–ï–°–°–ò–ù–ì–ê
# ============================================================================

def preprocess_kbd_for_opus(text):
    """–ó–∞–º–µ–Ω–∞ ”Ä ‚Üí I –¥–ª—è Opus-MT"""
    return text.replace('”Ä', 'I').replace('”è', 'I') if isinstance(text, str) else text

def postprocess_kbd_from_opus(text):
    """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ I ‚Üí ”Ä –∏–∑ Opus-MT"""
    return text.replace('I', '”Ä') if isinstance(text, str) else text

def preprocess_kbd_for_m2m100(text):
    """–ó–∞–º–µ–Ω–∞ ”Ä ‚Üí I –¥–ª—è M2M100"""
    return text.replace('”Ä', 'I').replace('”è', 'I') if isinstance(text, str) else text

def postprocess_kbd_from_m2m100(text):
    """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ I ‚Üí ”Ä –∏–∑ M2M100"""
    return text.replace('I', '”Ä') if isinstance(text, str) else text

# ============================================================================
# –ó–ê–ì–†–£–ó–ö–ê –¢–ï–°–¢–û–í–´–• –î–ê–ù–ù–´–• –ò–ó –ö–û–†–ü–£–°–ê
# ============================================================================

def load_test_samples_from_corpus():
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã –Ω–∞–ø—Ä—è–º—É—é –∏–∑ –∫–æ—Ä–ø—É—Å–∞.
    –ë–µ—Ä–µ—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ 100 –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è.
    """
    print("\n" + "="*80)
    print("üì• –ó–ê–ì–†–£–ó–ö–ê –¢–ï–°–¢–û–í–´–• –î–ê–ù–ù–´–• –ò–ó –ö–û–†–ü–£–°–ê")
    print("="*80)
    
    corpus_path = Path("data/circassian_corpus")
    
    if not corpus_path.exists():
        raise FileNotFoundError(f"–ö–æ—Ä–ø—É—Å –Ω–µ –Ω–∞–π–¥–µ–Ω: {corpus_path}")
    
    print(f"üìÇ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–æ—Ä–ø—É—Å–∞:")
    for item in corpus_path.iterdir():
        if item.is_dir():
            print(f"   üìÅ {item.name}")
    
    # –§—É–Ω–∫—Ü–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–∞—Ä –∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
    def extract_pairs_from_dataset(dataset_path, source_key='ru', target_key='kbd'):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–∞—Ä—ã –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø–æ –ø—É—Ç–∏"""
        try:
            dataset = load_from_disk(dataset_path)
            pairs = []
            
            for item in dataset:
                try:
                    if isinstance(item, dict) and 'translation' in item:
                        translation = item['translation']
                    else:
                        translation = item
                    
                    if isinstance(translation, str):
                        try:
                            parsed = json.loads(translation)
                        except:
                            continue
                    else:
                        parsed = translation
                    
                    if isinstance(parsed, dict) and source_key in parsed and target_key in parsed:
                        source_text = str(parsed[source_key]).strip()
                        target_text = str(parsed[target_key]).strip()
                        
                        if source_text and target_text and source_text != 'None' and target_text != 'None':
                            pairs.append({
                                'source': source_text,
                                'target': target_text
                            })
                except Exception as e:
                    continue
            
            return pairs
            
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {dataset_path}: {e}")
            return []
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –ø–∞–ø–æ–∫
    print("\nüîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö RU‚ÜíKBD...")
    ru_kbd_path = corpus_path / "ru_kbd"
    ru_kbd_pairs = extract_pairs_from_dataset(ru_kbd_path, source_key='ru', target_key='kbd')
    
    print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö KBD‚ÜíRU...")
    kbd_ru_path = corpus_path / "kbd_ru"
    kbd_ru_pairs = extract_pairs_from_dataset(kbd_ru_path, source_key='kbd', target_key='ru')
    
    print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ –ø–∞—Ä:")
    print(f"   RU‚ÜíKBD: {len(ru_kbd_pairs):,}")
    print(f"   KBD‚ÜíRU: {len(kbd_ru_pairs):,}")
    
    if len(ru_kbd_pairs) == 0 or len(kbd_ru_pairs) == 0:
        print("‚ö†Ô∏è  –í–Ω–∏–º–∞–Ω–∏–µ: –æ–¥–∏–Ω –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –ø—É—Å—Ç!")
    
    # –í—ã–±–∏—Ä–∞–µ–º 100 —Å–ª—É—á–∞–π–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º
    def select_samples(pairs, n=TEST_SIZE, interval=SAMPLE_INTERVAL):
        """–í—ã–±–∏—Ä–∞–µ—Ç N –ø—Ä–∏–º–µ—Ä–æ–≤ —Å –∑–∞–¥–∞–Ω–Ω—ã–º –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º"""
        if not pairs:
            return []
            
        if len(pairs) < n * interval:
            # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–∞–ª–æ, –±–µ—Ä–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ
            indices = random.sample(range(len(pairs)), min(n, len(pairs)))
        else:
            # –ë–µ—Ä–µ–º —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º –¥–ª—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è
            indices = [i * interval for i in range(n) if i * interval < len(pairs)]
            # –î–æ–ø–æ–ª–Ω—è–µ–º —Å–ª—É—á–∞–π–Ω—ã–º–∏ –µ—Å–ª–∏ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç
            if len(indices) < n:
                remaining = n - len(indices)
                extra_indices = random.sample(
                    [i for i in range(len(pairs)) if i not in indices],
                    remaining
                )
                indices.extend(extra_indices)
        
        return [pairs[i] for i in sorted(indices)]
    
    ru_kbd_test = select_samples(ru_kbd_pairs)
    kbd_ru_test = select_samples(kbd_ru_pairs)
    
    print(f"\n‚úÖ –í—ã–±—Ä–∞–Ω–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤:")
    print(f"   RU‚ÜíKBD: {len(ru_kbd_test)}")
    print(f"   KBD‚ÜíRU: {len(kbd_ru_test)}")
    
    # –ü—Ä–∏–º–µ—Ä—ã
    if ru_kbd_test:
        print(f"\nüìù –ü—Ä–∏–º–µ—Ä—ã RU‚ÜíKBD:")
        for i in range(min(3, len(ru_kbd_test))):
            print(f"   {i+1}. RU:  {ru_kbd_test[i]['source'][:60]}...")
            print(f"      KBD: {ru_kbd_test[i]['target'][:60]}...")
    
    if kbd_ru_test:
        print(f"\nüìù –ü—Ä–∏–º–µ—Ä—ã KBD‚ÜíRU:")
        for i in range(min(3, len(kbd_ru_test))):
            print(f"   {i+1}. KBD: {kbd_ru_test[i]['source'][:60]}...")
            print(f"      RU:  {kbd_ru_test[i]['target'][:60]}...")
    
    return {
        'ru_kbd': ru_kbd_test,
        'kbd_ru': kbd_ru_test
    }

# ============================================================================
# –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ï–ô
# ============================================================================

def load_model_and_tokenizer(model_key, model_config):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä"""
    model_path = Path(model_config['path'])
    
    if not model_path.exists():
        print(f"   ‚ö†Ô∏è  –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}")
        return None, None
    
    try:
        if model_config['type'] == 'opus':
            tokenizer = MarianTokenizer.from_pretrained(model_path)
            model = MarianMTModel.from_pretrained(
                model_path,
                torch_dtype=torch.float32
            ).to(device)
        else:  # m2m100
            tokenizer = AutoTokenizer.from_pretrained(model_path)
            model = AutoModelForSeq2SeqLM.from_pretrained(
                model_path,
                torch_dtype=torch.float32
            ).to(device)
        
        model.eval()
        print(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {model_config['name']}")
        return model, tokenizer
    
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {model_key}: {e}")
        return None, None

# ============================================================================
# –§–£–ù–ö–¶–ò–ò –ü–ï–†–ï–í–û–î–ê
# ============================================================================

def translate_opus_ru_kbd(model, tokenizer, text):
    """–ü–µ—Ä–µ–≤–æ–¥ RU‚ÜíKBD –¥–ª—è Opus-MT"""
    inputs = tokenizer(text, return_tensors="pt", max_length=128, truncation=True).to(device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=128,
            num_beams=4,
            early_stopping=True,
            repetition_penalty=1.2
        )
    
    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return postprocess_kbd_from_opus(translation)

def translate_opus_kbd_ru(model, tokenizer, text):
    """–ü–µ—Ä–µ–≤–æ–¥ KBD‚ÜíRU –¥–ª—è Opus-MT"""
    processed_text = preprocess_kbd_for_opus(text)
    inputs = tokenizer(processed_text, return_tensors="pt", max_length=128, truncation=True).to(device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=128,
            num_beams=4,
            early_stopping=True,
            repetition_penalty=1.2
        )
    
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

def translate_m2m100_ru_kbd(model, tokenizer, text):
    """–ü–µ—Ä–µ–≤–æ–¥ RU‚ÜíKBD –¥–ª—è M2M100"""
    formatted_input = f"__zu__ {text}"
    inputs = tokenizer(formatted_input, return_tensors="pt", max_length=128, truncation=True).to(device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=128,
            num_beams=4,
            early_stopping=True,
            repetition_penalty=1.2
        )
    
    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return postprocess_kbd_from_m2m100(translation)

def translate_m2m100_kbd_ru(model, tokenizer, text):
    """–ü–µ—Ä–µ–≤–æ–¥ KBD‚ÜíRU –¥–ª—è M2M100"""
    processed_text = preprocess_kbd_for_m2m100(text)
    formatted_input = f"__ru__ {processed_text}"
    inputs = tokenizer(formatted_input, return_tensors="pt", max_length=128, truncation=True).to(device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=128,
            num_beams=4,
            early_stopping=True,
            repetition_penalty=1.2
        )
    
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# –°–ª–æ–≤–∞—Ä—å —Ñ—É–Ω–∫—Ü–∏–π –ø–µ—Ä–µ–≤–æ–¥–∞
TRANSLATE_FUNCS = {
    "opus_ru_kbd": translate_opus_ru_kbd,
    "opus_kbd_ru": translate_opus_kbd_ru,
    "m2m100_ru_kbd": translate_m2m100_ru_kbd,
    "m2m100_kbd_ru": translate_m2m100_kbd_ru
}

# ============================================================================
# –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò
# ============================================================================

def test_model(model_key, model_config, test_data):
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –æ–¥–Ω—É –º–æ–¥–µ–ª—å"""
    print(f"\n{'='*80}")
    print(f"üîç –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï: {model_config['name']}")
    print(f"{'='*80}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    model, tokenizer = load_model_and_tokenizer(model_key, model_config)
    
    if model is None or tokenizer is None:
        print(f"   ‚ùå –ü—Ä–æ–ø—É—Å–∫ {model_key}")
        return None
    
    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—é
    direction = model_config['direction']
    test_examples = test_data[direction]
    
    if not test_examples:
        print(f"   ‚ùå –ù–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è: {direction}")
        return None
    
    print(f"üìä –¢–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {len(test_examples)}")
    
    # –§—É–Ω–∫—Ü–∏—è –ø–µ—Ä–µ–≤–æ–¥–∞
    translate_func = TRANSLATE_FUNCS[model_key]
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    sacrebleu = evaluate.load("sacrebleu")
    chrf = evaluate.load("chrf")
    ter = evaluate.load("ter")
    
    predictions = []
    references = []
    translation_times = []
    
    print("üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–µ—Ä–µ–≤–æ–¥–æ–≤...")
    start_time = time.time()
    
    for i, example in enumerate(test_examples):
        try:
            source_text = example['source']
            target_text = example['target']
            
            # –ó–∞—Å–µ–∫–∞–µ–º –≤—Ä–µ–º—è –ø–µ—Ä–µ–≤–æ–¥–∞
            trans_start = time.time()
            prediction = translate_func(model, tokenizer, source_text)
            trans_time = time.time() - trans_start
            
            predictions.append(prediction)
            references.append([target_text])
            translation_times.append(trans_time)
            
            if (i + 1) % 20 == 0:
                speed = (i + 1) / (time.time() - start_time)
                print(f"   {i+1}/{len(test_examples)} ({speed:.1f} ex/s)")
        
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –≤ –ø—Ä–∏–º–µ—Ä–µ {i}: {e}")
            continue
    
    total_time = time.time() - start_time
    
    if not predictions:
        print("   ‚ùå –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")
        return None
    
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    print("\nüìä –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫...")
    
    bleu_result = sacrebleu.compute(predictions=predictions, references=references)
    chrf_result = chrf.compute(predictions=predictions, references=references)
    ter_result = ter.compute(predictions=predictions, references=references)
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    exact_matches = sum(1 for p, r in zip(predictions, references) if p == r[0])
    exact_match_rate = (exact_matches / len(predictions)) * 100
    
    avg_trans_time = np.mean(translation_times)
    std_trans_time = np.std(translation_times)
    
    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print(f"\n‚úÖ –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print(f"   BLEU:              {bleu_result['score']:.2f}")
    print(f"   chrF:              {chrf_result['score']:.2f}")
    print(f"   chrF++:            {chrf_result.get('score', 0):.2f}")
    print(f"   TER:               {ter_result['score']:.2f}")
    print(f"   –¢–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è: {exact_match_rate:.1f}%")
    print(f"   –í—Ä–µ–º—è –Ω–∞ –ø—Ä–∏–º–µ—Ä:   {avg_trans_time*1000:.1f} ¬± {std_trans_time*1000:.1f} –º—Å")
    print(f"   –û–±—â–µ–µ –≤—Ä–µ–º—è:       {total_time:.1f}—Å")
    print(f"   –°–∫–æ—Ä–æ—Å—Ç—å:          {len(predictions)/total_time:.1f} ex/s")
    
    # –ü—Ä–∏–º–µ—Ä—ã –ø–µ—Ä–µ–≤–æ–¥–æ–≤
    print(f"\nüìù –ü–†–ò–ú–ï–†–´ –ü–ï–†–ï–í–û–î–û–í:")
    for i in range(min(5, len(predictions))):
        example = test_examples[i]
        match = "‚úÖ" if predictions[i] == references[i][0] else "‚ùå"
        
        print(f"\n   {i+1}. {match}")
        print(f"      –ò—Å—Ç–æ—á–Ω–∏–∫:  {example['source'][:70]}")
        print(f"      –û–∂–∏–¥–∞–ª–æ—Å—å: {references[i][0][:70]}")
        print(f"      –ü–æ–ª—É—á–µ–Ω–æ:  {predictions[i][:70]}")
    
    return {
        "model": model_config['name'],
        "author": model_config['author'],
        "base_model": model_config['base_model'],
        "direction": direction,
        "bleu": round(bleu_result['score'], 2),
        "chrf": round(chrf_result['score'], 2),
        "ter": round(ter_result['score'], 2),
        "exact_match_rate": round(exact_match_rate, 1),
        "avg_time_ms": round(avg_trans_time * 1000, 1),
        "std_time_ms": round(std_trans_time * 1000, 1),
        "total_time": round(total_time, 1),
        "speed": round(len(predictions) / total_time, 1),
        "examples": len(predictions),
        "predictions": predictions[:10],  # –ü–µ—Ä–≤—ã–µ 10 –¥–ª—è –æ—Ç—á–µ—Ç–∞
        "references": [r[0] for r in references[:10]]
    }

# ============================================================================
# –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø
# ============================================================================

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –±–µ–Ω—á–º–∞—Ä–∫–∞"""
    
    print(f"\nüìã –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –¢–ï–°–¢–ê:")
    print(f"   –¢–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {TEST_SIZE} –Ω–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ")
    print(f"   –ò–Ω—Ç–µ—Ä–≤–∞–ª –≤—ã–±–æ—Ä–∫–∏:  –∫–∞–∂–¥–æ–µ {SAMPLE_INTERVAL}-–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ")
    print(f"   Seed:              {SEED}")
    print(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ:        {device}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    try:
        test_data = load_test_samples_from_corpus()
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
    all_results = {}
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—Ç–¥–µ–ª—å–Ω–æ
    for direction, models_in_direction in MODELS.items():
        print(f"\n{'='*80}")
        print(f"üîç –ù–ê–ü–†–ê–í–õ–ï–ù–ò–ï: {direction.upper()}")
        print(f"{'='*80}")
        
        for model_key, model_config in models_in_direction.items():
            try:
                result = test_model(model_key, model_config, test_data)
                if result:
                    all_results[model_key] = result
            except Exception as e:
                print(f"\n‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è {model_key}: {e}")
                continue
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if not all_results:
        print("\n‚ùå –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
        return
        
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # JSON —Å –ø–æ–ª–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    results_file = f"benchmark_results_{timestamp}.json"
    with open(results_file, "w", encoding="utf-8") as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_file}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤—ã—Ö —Ç–∞–±–ª–∏—Ü –∏ –æ—Ç—á–µ—Ç–∞
    create_comparison_report(all_results, test_data)

# ============================================================================
# –°–û–ó–î–ê–ù–ò–ï –û–¢–ß–ï–¢–ê
# ============================================================================

def create_comparison_report(all_results, test_data):
    """–°–æ–∑–¥–∞–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
    
    print(f"\n{'='*80}")
    print(f"üèÜ –°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–´–ô –û–¢–ß–ï–¢")
    print(f"{'='*80}")
    
    if not all_results:
        print("‚ùå –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
        return
    
    # –¢–∞–±–ª–∏—Ü–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –¥–ª—è RU‚ÜíKBD
    ru_kbd_results = {k: v for k, v in all_results.items() if k in ['opus_ru_kbd', 'm2m100_ru_kbd']}
    
    if ru_kbd_results:
        print(f"\nüîπ –†–£–°–°–ö–ò–ô ‚Üí –ö–ê–ë–ê–†–î–ò–ù–°–ö–ò–ô ({len(test_data['ru_kbd'])} –ø—Ä–∏–º–µ—Ä–æ–≤)")
        print("="*80)
        
        df_ru_kbd = pd.DataFrame([
            {
                "–ú–æ–¥–µ–ª—å": r['model'],
                "–ê–≤—Ç–æ—Ä": r['author'],
                "BLEU": r['bleu'],
                "chrF": r['chrf'],
                "TER": r['ter'],
                "–¢–æ—á–Ω—ã–µ %": r['exact_match_rate'],
                "–í—Ä–µ–º—è (–º—Å)": f"{r['avg_time_ms']:.1f}¬±{r['std_time_ms']:.1f}",
                "–°–∫–æ—Ä–æ—Å—Ç—å": f"{r['speed']:.1f} ex/s"
            }
            for r in sorted(ru_kbd_results.values(), key=lambda x: x['bleu'], reverse=True)
        ])
        
        print(df_ru_kbd.to_string(index=False))
        
        # –ü–æ–±–µ–¥–∏—Ç–µ–ª—å
        if ru_kbd_results:
            best_ru_kbd = max(ru_kbd_results.values(), key=lambda x: x['bleu'])
            print(f"\nü•á –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_ru_kbd['model']} (BLEU: {best_ru_kbd['bleu']})")
    
    # –¢–∞–±–ª–∏—Ü–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –¥–ª—è KBD‚ÜíRU
    kbd_ru_results = {k: v for k, v in all_results.items() if k in ['opus_kbd_ru', 'm2m100_kbd_ru']}
    
    if kbd_ru_results:
        print(f"\nüîπ –ö–ê–ë–ê–†–î–ò–ù–°–ö–ò–ô ‚Üí –†–£–°–°–ö–ò–ô ({len(test_data['kbd_ru'])} –ø—Ä–∏–º–µ—Ä–æ–≤)")
        print("="*80)
        
        df_kbd_ru = pd.DataFrame([
            {
                "–ú–æ–¥–µ–ª—å": r['model'],
                "–ê–≤—Ç–æ—Ä": r['author'],
                "BLEU": r['bleu'],
                "chrF": r['chrf'],
                "TER": r['ter'],
                "–¢–æ—á–Ω—ã–µ %": r['exact_match_rate'],
                "–í—Ä–µ–º—è (–º—Å)": f"{r['avg_time_ms']:.1f}¬±{r['std_time_ms']:.1f}",
                "–°–∫–æ—Ä–æ—Å—Ç—å": f"{r['speed']:.1f} ex/s"
            }
            for r in sorted(kbd_ru_results.values(), key=lambda x: x['bleu'], reverse=True)
        ])
        
        print(df_kbd_ru.to_string(index=False))
        
        # –ü–æ–±–µ–¥–∏—Ç–µ–ª—å
        if kbd_ru_results:
            best_kbd_ru = max(kbd_ru_results.values(), key=lambda x: x['bleu'])
            print(f"\nü•á –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_kbd_ru['model']} (BLEU: {best_kbd_ru['bleu']})")
    

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ Markdown –æ—Ç—á–µ—Ç–∞ –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
    create_markdown_report(all_results, test_data, timestamp)

def create_markdown_report(all_results, test_data, timestamp):
    """–°–æ–∑–¥–∞–µ—Ç Markdown –æ—Ç—á–µ—Ç –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –Ω–∞ HuggingFace"""
    
    report = f"""# Benchmark Results - Russian‚ÜîKabardian Translation Models

**Date:** {datetime.now().strftime("%Y-%m-%d %H:%M")}
**Test Size:** {TEST_SIZE} examples per direction
**Dataset:** [adiga-ai/circassian-parallel-corpus](https://huggingface.co/datasets/adiga-ai/circassian-parallel-corpus)
**Device:** {device}

## Methodology

- **Test Set:** {TEST_SIZE} randomly sampled sentences from the corpus (every {SAMPLE_INTERVAL}th sentence)
- **Metrics:** BLEU (SacreBLEU), chrF, TER, Exact Match Rate
- **Generation Parameters:** beam_search (num_beams=4), max_length=128
- **Reproducibility:** seed={SEED}

## Results

### Russian ‚Üí Kabardian

"""
    
    # –¢–∞–±–ª–∏—Ü–∞ RU‚ÜíKBD
    ru_kbd_results = {k: v for k, v in all_results.items() if k in ['opus_ru_kbd', 'm2m100_ru_kbd']}
    
    if ru_kbd_results:
        report += "| Model | Author | BLEU | chrF | TER | Exact Match | Avg Time (ms) | Speed (ex/s) |\n"
        report += "|-------|--------|------|------|-----|-------------|---------------|---------------|\n"
        
        for r in sorted(ru_kbd_results.values(), key=lambda x: x['bleu'], reverse=True):
            report += f"| {r['model']} | {r['author']} | **{r['bleu']}** | {r['chrf']} | {r['ter']} | {r['exact_match_rate']}% | {r['avg_time_ms']:.1f} | {r['speed']:.1f} |\n"
        
        # –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å
        if ru_kbd_results:
            best = max(ru_kbd_results.values(), key=lambda x: x['bleu'])
            report += f"\n**Winner:** {best['model']} with BLEU {best['bleu']}\n"
    
    report += "\n### Kabardian ‚Üí Russian\n\n"
    
    # –¢–∞–±–ª–∏—Ü–∞ KBD‚ÜíRU
    kbd_ru_results = {k: v for k, v in all_results.items() if k in ['opus_kbd_ru', 'm2m100_kbd_ru']}
    
    if kbd_ru_results:
        report += "| Model | Author | BLEU | chrF | TER | Exact Match | Avg Time (ms) | Speed (ex/s) |\n"
        report += "|-------|--------|------|------|-----|-------------|---------------|---------------|\n"
        
        for r in sorted(kbd_ru_results.values(), key=lambda x: x['bleu'], reverse=True):
            report += f"| {r['model']} | {r['author']} | **{r['bleu']}** | {r['chrf']} | {r['ter']} | {r['exact_match_rate']}% | {r['avg_time_ms']:.1f} | {r['speed']:.1f} |\n"
        
        # –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å
        if kbd_ru_results:
            best = max(kbd_ru_results.values(), key=lambda x: x['bleu'])
            report += f"\n**Winner:** {best['model']} with BLEU {best['bleu']}\n"
    
    # –ü—Ä–∏–º–µ—Ä—ã –ø–µ—Ä–µ–≤–æ–¥–æ–≤
    report += "\n## Translation Examples\n\n"
    
    for model_key, result in all_results.items():
        report += f"\n### {result['model']}\n\n"
        report += "| Source | Reference | Translation |\n"
        report += "|--------|-----------|-------------|\n"
        
        for i in range(min(5, len(result['predictions']))):
            source = test_data[result['direction']][i]['source'][:50]
            ref = result['references'][i][:50]
            pred = result['predictions'][i][:50]
            report += f"| {source}... | {ref}... | {pred}... |\n"
    
    # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    report += "\n## Interpretation\n\n"
    report += "### Metrics Explained\n\n"
    report += "- **BLEU**: Measures n-gram overlap (0-100, higher is better)\n"
    report += "- **chrF**: Character-level F-score (0-100, higher is better)\n"
    report += "- **TER**: Translation Error Rate (0-100, lower is better)\n"
    report += "- **Exact Match**: Percentage of perfect translations\n\n"
    
    report += "### Quality Assessment\n\n"
    
    # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    all_bleu = [r['bleu'] for r in all_results.values()]
    avg_bleu = sum(all_bleu) / len(all_bleu) if all_bleu else 0
    
    if avg_bleu > 30:
        quality = "Excellent - suitable for production use"
    elif avg_bleu > 25:
        quality = "Good - suitable for most applications"
    elif avg_bleu > 20:
        quality = "Acceptable - suitable for basic translation tasks"
    else:
        quality = "Needs improvement - requires additional training"
    
    report += f"Average BLEU across all models: **{avg_bleu:.2f}**\n\n"
    report += f"Quality Assessment: **{quality}**\n\n"
    
    report += "## Technical Details\n\n"
    report += f"- Test conducted on: {device.upper()}\n"
    report += f"- Framework: PyTorch + Transformers\n"
    report += f"- Reproducible with seed: {SEED}\n"
    report += f"- Dataset splits: Sampled every {SAMPLE_INTERVAL}th example\n"
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
    report_file = f"BENCHMARK_REPORT_{timestamp}.md"
    with open(report_file, "w", encoding="utf-8") as f:
        f.write(report)
    
    print(f"üíæ Markdown –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_file}")
    print(f"   –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –µ–≥–æ –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –Ω–∞ HuggingFace!")

if __name__ == "__main__":
    main()
    
    print(f"\n{'='*80}")
    print("‚úÖ –ë–ï–ù–ß–ú–ê–†–ö –ó–ê–í–ï–†–®–ï–ù!")
    print("="*80)
    print("\nüìÅ –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
    print("   ‚Ä¢ benchmark_results_*.json - –ø–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
    print("   ‚Ä¢ benchmark_ru_kbd_*.csv - —Ç–∞–±–ª–∏—Ü–∞ RU‚ÜíKBD")
    print("   ‚Ä¢ benchmark_kbd_ru_*.csv - —Ç–∞–±–ª–∏—Ü–∞ KBD‚ÜíRU")
    print("   ‚Ä¢ BENCHMARK_REPORT_*.md - –æ—Ç—á–µ—Ç –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏")
    print("\nüöÄ –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
    print("   1. –î–æ–±–∞–≤—å—Ç–µ BENCHMARK_REPORT_*.md –≤ Model Card –Ω–∞ HuggingFace")
    print("   2. –†–∞–∑–º–µ—Å—Ç–∏—Ç–µ —Ç–∞–±–ª–∏—Ü—ã CSV –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏")
    print("   3. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å baseline")
    print("\nüí° –î–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞: python benchmark_models.py")
    print("="*80)